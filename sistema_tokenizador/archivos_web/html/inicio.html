<!--
  Html de página de inicio de sitio público.
  Aplicación web de sistema tokenizador.
  Proyecto Lovelace.
-->

<div
  class="seccion"
  flex="none"
  layout="column">

  <h3>¿Qué es la tokenización?</h3>
    <h4>Primero, ¿qué es un token?</h4>
    <p>
      Antes de definir la <em>tokenización</em>, es menester definir lo que es un
      token: normalmente, se le conoce como un valor representativo de otro valor;
      el concepto de token es ampliamente utilizado en otros contextos, y,
      dependiendo de este, varían sus características. En la criptografía (que es
      el contexto en el que se encuentra este servicio), este <em>valor
      representativo</em> no debe tener una relación directa con el valor
      original; de hecho, que un adversario posea un token no representa peligro
      alguno, pues no es posible, sin los mecanismos de detokenización, regresar
      al valor original. Por lo tanto, un token puede ser utilizado para
      proteger información confidencial.
    </p>

    <h4>Sobre la tokenización</h4>
    <p>
      Con esta definición de un token, se puede definir a la tokenización como el
      proceso en el que se obtiene el valor sustituto. Este paradigma de sustituir
      datos sensibles por datos representativos (y sin valor) es relativamente
      nuevo, pero muy útil al momento de proteger la información, pues permite
      concentrar toda la información valiosa en un solo lugar y dejar de
      preocuparse por protegerla en todos lados (en vez de vigilar un castillo,
      se vigila una habitación).
    </p>
    <p>
      La operación inversa, es decir dado un token, obtener el PAN que le
      corresponde, es llamada <em>detokenización</em>.
    </p>

  <h3>¿Por qué utilizar un servicio de tokenización?</h3>
  <p>
    Desde que el comercio eletrónico comenzó a popularizarse, los fraudes
    relacionados con las tarjetas bancarias y el comercio en línea crecieron,
    pues los sitios de bancarios y de comercion no estaban preparados para la
    demanda y las amenazas de seguridad a las que comenzaron a enfrentarse. Por
    lo tanto, en 2004, fue publicado por primera vez un estándar (PCI DSS,
    respaldado por compañías como VISA y MasterCard) donde se dan las guías de
    seguridad que deberían seguir todos aquellos que traten con datos de
    tarjetas o realicen transacciones bancarias.
  </p>
  <p>
    Aunque este estándar no es obligatorio para aquellas entidades que
    realicen menos de 20 000 transacciones en un año; es extremadamente
    recomendable que se sigan la guía para mantener segura la información y 
    transacciones.
    Empero, satisfacer completamente el estándar es una verdadera proeza, cuenta
    con más de 200 requerimientos para segurar la robustez y seguridad del
    sistema que trata con los datos y pagos con tarjetas bancarias.
  </p>
  <p>
    Es aquí donde entran los servicios de tokenización, pues se encargan de
    cumplir con todos los requerimientos y quitarle esa pesada carga para que
    pueda enfocarse en sus actividades sin preocuparse por el estándar y sus
    implementaciones. Obviamente, es muy importante saber cómo es que servicio
    de tokenización está obteniendo los tokens, pues no basta con asegurar que
    son seguros; los servicios deben indicar (y hacer públicos) los algoritmos
    que utilizan para generar los tokens. Por lo tanto, que un servicio
    clame que tiene la mejor manera para generar tokens, pero no especifique
    cómo es que los está obteniendo (o que lo haga de manera vaga, por ejemplo,
    asegurando que sus tokens son obtenidos aleatoriamente sin dar más
    detalles), es inadmisible, pues un buen algoritmo tokenizador no depende en
    absoluto de ser mantenido en secreto.
  </p>
  
  <h3>¿Por qué utilizar ESTE servicio de tokenización?</h3>
  <p>
    A diferencia de otros competidores, nosotros somos transparentes respecto
    a la manera en la que se generan los tokens; actualmente, hay cinco 
    algoritmos distintos para obtener tokens y usted puede decidir, para cada
    token, el algoritmo que será utilizado: un poco más adelante hay una sección
    que explica brevemente cada algoritmo y nuestras implementaciones son
    públicas. 
  </p>

  <h3>¿Qué es el PCI SSC?</h3>
  <p>
    El <a href="https://www.pcisecuritystandards.org/pci_security/">Payment
    Card Industry Security Standards Council (PCI SSC)</a> es una organización
    internacional encargada de estandarizar, desarrollar e informar sobre
    cómo hacer transacciones bancarias seguras.
  </p>
  <p>
    Esta organización desarrolló el PCI Data Security Standard (DSS): un
    estándar en el que se indica la manera de mantener la información bancaria
    segura mediante la implementación de protocolos de seguridad.
  </p>
  <p>
    Es importante resaltar que el PCI SSC ha establecido como requerimiento la
    implementación del PCI DSS solo para aquellas entidades que realizan un
    número de transacciones considerables (20 000 al año).
  </p>

  <h3>Sobre los algoritmos de tokenización</h3>
  <p>
    Nuestro sistema permite seleccionar el algoritmo con el que se desea
    tokenizar un dato. Se tienen dos tipos de algoritmos: los reversibles y los
    irreversibles. La diferencia entre unos y otros radica en la operación de
    detokenización, pues los primeros (los reversibles) <em>descifran</em> el
    token para obtener el PAN; mientras que los segundos hacen una consulta a
    una base de datos para obtener el PAN.
  </p>
  <p>
    Contamos con 2 algoritmos de tokenización reversibles: FFX y BPS, y 3
    algoritmos de tokenización irreversibles: TKR, AHR y DRBG. A continuación
    se encuentra una breve descripción de cada uno.
  </p>

  <div>
    <h4>FFX (Format-preserving Feistel-based Encryption) o FF1</h4>
    <p>
      Este es un cifrado que permite cifrar cadenas de cualquier tamaño,
      compuestas por cualquier tipo de caracteres, el cual es usado para
      tokenizar. Fue publicado por Mihir Bellare, Phillip Rogaway y
      Terence Spies en 2009, aunque en 2016 el National Institute of
      Standards and Technology (NIST) le dio el nombre de FF1 a este
      algoritmo.
    </p>
    <p>
      De forma general, el algoritmo usa redes Feistel junto con primitivas
      criptográficas (función hash o cifrados por bloques) adaptadas
      en la función de ronda de la red para lograr preservar el formato
      del dato dado para tokenizar; esto significa que el token tendrá
      la misma longitud y se mantendrá en el mismo dominio que el dato
      original.
    </p>
    <p>
      Para conocer más sobre este algoritmo revise el siguiente artículo:
      <a
        href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.304.1736&rep=rep1&type=pdf">
        The FFX Mode of Operation for Format-PreservingEncryption
      </a>
    </p>
  </div>
  <div>
    <h4>BPS (Brier-Peyrin-Stern) o FF3</h4>
    <p>
      Es un cifrado que preserva el formato usado para tokenizar,
      propuesto por Eric Brier, Thomas Peyrin y Jacques Stern en 2010 y
      renombrado por el NIST como FF3.
    </p>
    <p>
      Al igual que FFX, usa redes Feistel y primitivas criptográficas para
      lograr que el token tenga el mismo formato que el dato original.
    </p>
    <p>
      Para conocer más sobre este algoritmo revise el siguiente artículo:
      <a
        href="https://pdfs.semanticscholar.org/0be5/d4c77e333d78ddab5c4bf55d15649a660771.pdf">
        BPS: a Format-Preserving Encryption Proposal
      </a>
    </p>
  </div>
  <div>
    <h4>TKR</h4>
    <p>
      Propuesto por Sandra Díaz Santiago, Lil María Rodríguez Henríquez y
      Debrup Chakraborty en 2016, es un algoritmo que usa primitivas
      criptográficas para generar tokens aleatorios y almacenarlos en una
      base de datos segura para guardar su relación con el dato original.
    </p>
    <p>
      Para conocer más sobre este algoritmo revisa el artículo:
      <a href="https://link.springer.com/article/10.1007%2Fs10207-015-0313-x">
        A cryptographic study of tokenization systems
      </a>
    </p>
  </div>
  <div>
    <h4>AHR (Algoritmo Híbrido Reversible)</h4>
    <p>
      Algoritmo de tokenización publicado por Riccardo Aragona, Riccardo
      Longo y Massimiliano Sala en 2017.
    </p>
    <p>
      Este se basa en un cifrado de bloques y una caminata cíclica para
      generar tokens que mantengan el formato; los tokens son almacenados en una
      base de datos segura para guardar la relación con los datos originales.
    </p>
    <p>
      Para conocer más sobre este algoritmo revise el siguiente artículo:
      <a href="https://link.springer.com/article/10.1007%2Fs00200-017-0313-3">
        Several proofs of security for a tokenization algorithm
      </a>
    </p>
  </div>
  <div>
    <h4>
      Tokenización por medio de un DRGB
      (Deterministic Random Bit Generator)
    </h4>
    <p>
      La tokenización por este medio se logra produciendo una cadena de
      bits aleatoria con un DRBG que se interpreta de forma especial para
      que tenga el formato del dato original.
    </p>
    <p>
      El DRGB puede estar basado tanto en funciones hash como en cifrados
      por bloque.
    </p>
  </div>

  <h3>Una comparación</h3>
  <p>
    En aras de arrojar luz sobre el desempeño de los algoritmos, a continuación
    se encuentran dos gráficas que muestran el tiempo que les toma a sendos
    algotimos realizar el mismo número de tokenizaciones. Naturalmente, los
    algoritmos irreversibles son más lentos, pues deben realizar operaciones
    en la base de datos; por lo que se comparan algoritmos reversibles con
    algoritmos reversibles y algoritmos irreversibles con algoritmos
    irreversibles.
  </p>
  <p>
    En esta primera gráfica se comparan los tiempos de tokenización y
    detokenización de los algoritmos de FFX y BPS mientras el número
    de operaciones se incrementa.
  </p>
  <div class="grafica">
    <img src="estaticos/imagenes/todo_rev.png">
  </div>
  <p>
    Es importante observar que los tiempos de tokenización y detokenización son
    tan parecidos, que sus gráficas se sobreponen completamente.
  </p>
  <p>
    En esta otra se muestra los tiempos de tokenización y detokenización de
    TKR, AHR y DRGB mientras el número de operaciones se incrementa.
  </p>
  <div class="grafica">
    <img src="estaticos/imagenes/todo_irrev.png">
  </div>
  <p>
    Aquí es importante resaltar que, si bien el tiempo de tokenización es
    considerable (comparándolo con los reversibles), el tiempo de detokenización
    es mucho más pequeño, incluso es menor a los algoritmos reversibles.
  </p>
</div>
